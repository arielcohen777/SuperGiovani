{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1012ac5",
   "metadata": {
    "papermill": {
     "duration": 0.005655,
     "end_time": "2025-04-19T16:28:03.133622",
     "exception": false,
     "start_time": "2025-04-19T16:28:03.127967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Socratic AI - Personalized Educational Assistant using Gemini\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Many students today rely on generative AI to quickly get direct answers, which can lead to passive learning and hinder the development of critical thinking skills. Traditional AI tutoring often provides immediate solutions, depriving learners of the opportunity to explore their own thought processes. This project addresses this challenge by transforming the educational experience into an active, self discovery process.\n",
    "\n",
    "## Mission Statement\n",
    "\n",
    "Develop a personalized Socratic tutor that guides students through a process of inquiry and reflection, leading them to discover solutions on their own, rather than simply being handed the answer. By constraining the AI to ask precise, open-ended questions, we aim to foster deeper understanding and encourage independent problem solving.\n",
    "\n",
    "## Use Case\n",
    "\n",
    "This project builds a personalized Socratic tutor using Google’s Gemini API. The tutor evaluates a student’s current knowledge through interactive quizzes and then engages them in a tailored dialogue. Instead of providing direct answers, the tutor uses carefully crafted questions to probe the student’s thinking, helping to expose gaps in understanding and promote meaningful learning outcomes.\n",
    "\n",
    "\n",
    "## Solution Overview\n",
    "\n",
    "1. The solution leverages the Gemini API’s powerful natural language capabilities along with structured output to create a dynamic and interactive educational tool:\n",
    "\n",
    "2. Quiz Generation:\n",
    "Gemini is used in JSON mode to automatically generate quizzes that assess a student’s grasp of key concepts. The structured quiz output ensures consistency and forms the baseline for personalized instruction.\n",
    "\n",
    "3. Socratic Dialogue:\n",
    "After the quiz, the tutor engages the student through a Socratic dialogue. This process employs constrained AI behavior—carefully tuned system instructions ensure that the tutor asks short, guided questions rather than delivering lengthy explanations. This constraint helps channel the student’s thinking toward exploring underlying principles rather than relying on straightforward answers.\n",
    "\n",
    "4. Context Maintenance:\n",
    "The system maintains the conversation history to retain context across multiple turns. This personalized context allows the AI tutor to adapt its line of questioning based on the student’s previous responses and demonstrated understanding.\n",
    "\n",
    "5. Evaluation and Feedback:\n",
    "An evaluation module uses a two-step process to generate detailed feedback on the tutoring interaction. The tutor’s response is first analyzed in verbose natural language and then distilled into a concise rating. This helps ensure that the feedback aligns with the educational goals and Socratic method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5650518",
   "metadata": {
    "papermill": {
     "duration": 0.004442,
     "end_time": "2025-04-19T16:28:03.143659",
     "exception": false,
     "start_time": "2025-04-19T16:28:03.139217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports and Setup\n",
    "\n",
    "This cell imports the necessary libraries and sets up the Gemini API client. It also retrieves the API key from Kaggle secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b5a6f2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:28:03.154443Z",
     "iopub.status.busy": "2025-04-19T16:28:03.154086Z",
     "iopub.status.idle": "2025-04-19T16:28:11.138820Z",
     "shell.execute_reply": "2025-04-19T16:28:11.137540Z"
    },
    "papermill": {
     "duration": 7.992439,
     "end_time": "2025-04-19T16:28:11.140897",
     "exception": false,
     "start_time": "2025-04-19T16:28:03.148458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: google-genai 0.2.2\r\n",
      "Uninstalling google-genai-0.2.2:\r\n",
      "  Successfully uninstalled google-genai-0.2.2\r\n",
      "Collecting google-genai==1.7.0\r\n",
      "  Downloading google_genai-1.7.0-py3-none-any.whl.metadata (32 kB)\r\n",
      "Collecting anyio<5.0.0,>=4.8.0 (from google-genai==1.7.0)\r\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\r\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-genai==1.7.0) (2.27.0)\r\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from google-genai==1.7.0) (0.28.1)\r\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-genai==1.7.0) (2.11.0a2)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.10/dist-packages (from google-genai==1.7.0) (2.32.3)\r\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from google-genai==1.7.0) (14.1)\r\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.10/dist-packages (from google-genai==1.7.0) (4.12.2)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai==1.7.0) (1.2.2)\r\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai==1.7.0) (3.10)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai==1.7.0) (1.3.1)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai==1.7.0) (5.5.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai==1.7.0) (0.4.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai==1.7.0) (4.9)\r\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai==1.7.0) (2025.1.31)\r\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai==1.7.0) (1.0.7)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai==1.7.0) (0.14.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai==1.7.0) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai==1.7.0) (2.29.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.28.1->google-genai==1.7.0) (3.4.1)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.28.1->google-genai==1.7.0) (2.3.0)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai==1.7.0) (0.6.1)\r\n",
      "Downloading google_genai-1.7.0-py3-none-any.whl (144 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading anyio-4.9.0-py3-none-any.whl (100 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: anyio, google-genai\r\n",
      "  Attempting uninstall: anyio\r\n",
      "    Found existing installation: anyio 3.7.1\r\n",
      "    Uninstalling anyio-3.7.1:\r\n",
      "      Successfully uninstalled anyio-3.7.1\r\n",
      "Successfully installed anyio-4.9.0 google-genai-1.7.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall google-genai -y\n",
    "!pip install google-genai==1.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef44fe9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-19T16:28:11.154271Z",
     "iopub.status.busy": "2025-04-19T16:28:11.153839Z",
     "iopub.status.idle": "2025-04-19T16:28:13.284952Z",
     "shell.execute_reply": "2025-04-19T16:28:13.283368Z"
    },
    "papermill": {
     "duration": 2.139924,
     "end_time": "2025-04-19T16:28:13.286928",
     "exception": false,
     "start_time": "2025-04-19T16:28:11.147004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from IPython.display import HTML, Markdown, display\n",
    "from google.api_core import retry\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "import typing_extensions as typing\n",
    "import json\n",
    "import pydantic\n",
    "import enum\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "print(genai.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dc8512",
   "metadata": {
    "papermill": {
     "duration": 0.006039,
     "end_time": "2025-04-19T16:28:13.298935",
     "exception": false,
     "start_time": "2025-04-19T16:28:13.292896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Structure for Chat History\n",
    "\n",
    "These classes define the structure for storing chat turns and their content. The `SimplePart` class is a simple wrapper for text content, and the `ChatTurn` class represents a single turn in the conversation, including the role (user or AI) and the content.\n",
    "\n",
    "These classes are used to maintain a structured history of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da93e507",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:28:13.312351Z",
     "iopub.status.busy": "2025-04-19T16:28:13.311747Z",
     "iopub.status.idle": "2025-04-19T16:28:13.318092Z",
     "shell.execute_reply": "2025-04-19T16:28:13.316912Z"
    },
    "papermill": {
     "duration": 0.015136,
     "end_time": "2025-04-19T16:28:13.320010",
     "exception": false,
     "start_time": "2025-04-19T16:28:13.304874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a simple Part-like class to wrap text.\n",
    "class SimplePart:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "# Update the ChatTurn class to use SimplePart for parts.\n",
    "class ChatTurn:\n",
    "    def __init__(self, role, content):\n",
    "        self.role = role\n",
    "        self.content = content\n",
    "        self.parts = [SimplePart(content)]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ChatTurn(role={self.role!r}, content={self.content!r})\"\n",
    "        \n",
    "# Color class console output\n",
    "class Colors:\n",
    "    RED = '\\033[91m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    BLUE = '\\033[94m'\n",
    "    RESET = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3a6732",
   "metadata": {
    "papermill": {
     "duration": 0.005789,
     "end_time": "2025-04-19T16:28:13.332418",
     "exception": false,
     "start_time": "2025-04-19T16:28:13.326629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Socratic Prompt - System Intructions\n",
    "\n",
    "This cell defines the core prompt that guides the AI's behavior. It outlines the AI's role as a Socratic tutor, provides guidelines for interaction, and includes example phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b8b7a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:28:13.345882Z",
     "iopub.status.busy": "2025-04-19T16:28:13.345495Z",
     "iopub.status.idle": "2025-04-19T16:28:13.875703Z",
     "shell.execute_reply": "2025-04-19T16:28:13.874385Z"
    },
    "papermill": {
     "duration": 0.539601,
     "end_time": "2025-04-19T16:28:13.877944",
     "exception": false,
     "start_time": "2025-04-19T16:28:13.338343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "socratic_prompt = \"\"\"\n",
    "Role: You are a thoughtful and patient tutor whose goal is to help the student master concepts through independent problem solving. Your job is to encourage critical thinking, provide detailed feedback, and promote metacognitive awareness.\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "1.  Never immediately provide the answer to the student's question.\n",
    "2.  First, ask the student to clearly explain what steps they've already taken and precisely where they're getting stuck.\n",
    "3.  Provide incremental hints or small nudges, not complete steps. Guide the student to think critically about the next logical action.\n",
    "4.  Error Analysis and Feedback:\n",
    "    -If the student makes a mistake, point out gently where and why the misunderstanding occurred.\n",
    "    -Explain the underlying misconceptions and provide clear, concise explanations.\n",
    "    -Provide the student with resources that can help them understand the problem better.\n",
    "5.  Metacognitive Prompts:\n",
    "    -Frequently prompt students to summarize their current understanding before moving forward.\n",
    "    -Ask students to reflect on their learning process and identify their strengths and weaknesses.\n",
    "    -Ask questions that promote self-evaluation, such as:\n",
    "        -\"What strategies did you use to solve this problem?\"\n",
    "        -\"What could you have done differently?\"\n",
    "        -\"How confident are you in your understanding of this concept?\"\n",
    "        -\"What are some areas where you feel you need more practice?\"\n",
    "6.  Adaptive Questioning:\n",
    "    -Adapt the difficulty of your questions based on the student's responses.\n",
    "    -If the student demonstrates a strong understanding, introduce more challenging questions.\n",
    "    -If the student is struggling, provide simpler questions and additional support.\n",
    "7.  If the student directly asks for the final solution, respectfully decline and instead redirect them by providing another hint or asking guiding questions.\n",
    "8.  Be encouraging and supportive throughout your interactions. Reinforce effort, progress, and persistence.\n",
    "\n",
    "Example phrases you can use:\n",
    "\n",
    "-   \"Can you explain your thinking so far?\"\n",
    "-   \"That's an interesting approach; what might you try next?\"\n",
    "-   \"You're on the right track, but check your previous step carefully, do you see anything unusual?\"\n",
    "-   \"Let’s slow down here. What information from the problem haven't you used yet?\"\n",
    "-   \"What are some strategies you used to come to that conclusion?\"\n",
    "-   \"Where do you think your understanding is breaking down?\"\n",
    "-   \"Explain this concept as if you were teaching it to a friend.\"\n",
    "\n",
    "Remember: your primary goal is to help the student develop problem-solving skills, confidence, and a deeper understanding, rather than simply providing solutions.\n",
    "\"\"\"\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=socratic_prompt),\n",
    "    contents=\"Hello there\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20139fcb",
   "metadata": {
    "papermill": {
     "duration": 0.005641,
     "end_time": "2025-04-19T16:28:13.889585",
     "exception": false,
     "start_time": "2025-04-19T16:28:13.883944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ask_agent Function\n",
    "\n",
    "This cell is responsible for sending user messages to the Gemini API, maintaining the conversation history, and returns the AI's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d995c9c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:28:13.903837Z",
     "iopub.status.busy": "2025-04-19T16:28:13.903417Z",
     "iopub.status.idle": "2025-04-19T16:28:13.909059Z",
     "shell.execute_reply": "2025-04-19T16:28:13.908061Z"
    },
    "papermill": {
     "duration": 0.015255,
     "end_time": "2025-04-19T16:28:13.910760",
     "exception": false,
     "start_time": "2025-04-19T16:28:13.895505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = []\n",
    "def ask_agent(user_message: str, model: str = \"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Send a user's message to the Gemini, maintain history and multi-turn chat,\n",
    "    then return the AI's response.\n",
    "    \"\"\"\n",
    "    # Append the user's message as a structured dict\n",
    "    history.append(ChatTurn(role=\"user\", content=user_message))\n",
    "    \n",
    "    # Create a new chat session (or update) with the current history\n",
    "    chat = client.chats.create(model=model, history=history)\n",
    "    \n",
    "    # Send the message to the model\n",
    "    agent_response = chat.send_message(user_message)\n",
    "    \n",
    "    # Convert the response to text\n",
    "    response_text = agent_response.text if hasattr(agent_response, \"text\") else str(agent_response)\n",
    "    \n",
    "    # Append the AI's response to the history\n",
    "    history.append(ChatTurn(role=\"model\", content=response_text))\n",
    "    \n",
    "    # Return the entire response object (so you can access .text, etc.)\n",
    "    return agent_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493cf2d4",
   "metadata": {
    "papermill": {
     "duration": 0.005623,
     "end_time": "2025-04-19T16:28:13.922449",
     "exception": false,
     "start_time": "2025-04-19T16:28:13.916826",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Typing Definitions for Quiz\n",
    "\n",
    "This cell defines the Pydantic models for quiz questions and quizzes, ensuring structured JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef352ca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:28:13.935752Z",
     "iopub.status.busy": "2025-04-19T16:28:13.935294Z",
     "iopub.status.idle": "2025-04-19T16:28:13.942075Z",
     "shell.execute_reply": "2025-04-19T16:28:13.941065Z"
    },
    "papermill": {
     "duration": 0.015447,
     "end_time": "2025-04-19T16:28:13.943847",
     "exception": false,
     "start_time": "2025-04-19T16:28:13.928400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QuizQuestion(pydantic.BaseModel):\n",
    "    question: str\n",
    "    options: list[str]\n",
    "    answer: str\n",
    "\n",
    "class Quiz(pydantic.BaseModel):\n",
    "    quiz: list[QuizQuestion]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc8170",
   "metadata": {
    "papermill": {
     "duration": 0.005903,
     "end_time": "2025-04-19T16:28:13.956442",
     "exception": false,
     "start_time": "2025-04-19T16:28:13.950539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Quiz Generation\n",
    "\n",
    "This function generates a quiz in JSON format using Gemini's JSON mode. It takes a topic as input and returns a list of quiz questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "559948a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:28:13.970231Z",
     "iopub.status.busy": "2025-04-19T16:28:13.969706Z",
     "iopub.status.idle": "2025-04-19T16:28:13.979278Z",
     "shell.execute_reply": "2025-04-19T16:28:13.978139Z"
    },
    "papermill": {
     "duration": 0.0188,
     "end_time": "2025-04-19T16:28:13.981385",
     "exception": false,
     "start_time": "2025-04-19T16:28:13.962585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_quiz(topic: str) -> list[QuizQuestion]:\n",
    "    \"\"\"\n",
    "    Uses Gemini's one shot 'generate_content' and JSON mode approach to produce a quiz in JSON format.\n",
    "    Returns a list of quiz questions.\n",
    "    \"\"\"\n",
    "\n",
    "    # 7 questions for now, but need to find a better way to evaluate?\n",
    "    quiz_generation_prompt = f\"\"\"\n",
    "    Generate a short quiz (7 questions) to assess the student's understanding of basic concepts about {topic}.\n",
    "    Let them know that your are first going to generate a short quiz to determin there understanding of the topic to help them further.\n",
    "    Structure the quiz in JSON format with the following schema:\n",
    "\n",
    "    {{\n",
    "        \"quiz\": [\n",
    "            {{\n",
    "                \"question\": \"...\", \n",
    "                \"options\": [\"...\", \"...\", \"...\", \"...\"], \n",
    "                \"answer\": \"...\"\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "\n",
    "    Each question must be multiple choice with exactly 4 options.\n",
    "    \"\"\"\n",
    "\n",
    "    # Manually construct the schema (had issues with JSON format earlier, solution for now)\n",
    "    quiz_schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"quiz\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"question\": {\"type\": \"string\"},\n",
    "                        \"options\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                        \"answer\": {\"type\": \"string\"}\n",
    "                    },\n",
    "                    \"required\": [\"question\", \"options\", \"answer\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"quiz\"]\n",
    "    }\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.0-flash',\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.1,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=quiz_schema, # Using the manual schema\n",
    "        ),\n",
    "        contents=quiz_generation_prompt\n",
    "    )\n",
    "\n",
    "    # Parse the JSON response\n",
    "    quiz_data = json.loads(response.text)\n",
    "    return [QuizQuestion(**q) for q in quiz_data[\"quiz\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22587ca1",
   "metadata": {
    "papermill": {
     "duration": 0.006139,
     "end_time": "2025-04-19T16:28:13.995920",
     "exception": false,
     "start_time": "2025-04-19T16:28:13.989781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## administer_quiz Function\n",
    "\n",
    "This cell is responsible for displaying the quiz questions in the console, collecting user answers, and returning a list of the user's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d41662ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:28:14.015010Z",
     "iopub.status.busy": "2025-04-19T16:28:14.014584Z",
     "iopub.status.idle": "2025-04-19T16:28:14.020971Z",
     "shell.execute_reply": "2025-04-19T16:28:14.019733Z"
    },
    "papermill": {
     "duration": 0.01974,
     "end_time": "2025-04-19T16:28:14.022944",
     "exception": false,
     "start_time": "2025-04-19T16:28:14.003204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def administer_quiz(quiz: list[QuizQuestion]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Display each question, gather user input for each.\n",
    "    \"\"\"\n",
    "    user_answers = []\n",
    "    \n",
    "    print(f\"{Colors.GREEN}Hi! We will  start with a short quiz to gauge your current understanding:\\n{Colors.RESET}\")\n",
    "    \n",
    "    for i, q in enumerate(quiz):\n",
    "        print(f\"{Colors.GREEN}Q{i+1}: {q.question}{Colors.RESET}\")\n",
    "        for j, opt in enumerate(q.options):\n",
    "            print(f\"{Colors.GREEN}    {j+1}. {opt}{Colors.RESET}\")\n",
    "        choice = input(f\"{Colors.BLUE}Enter the number of your choice: {Colors.RESET}\")\n",
    "        \n",
    "        try:\n",
    "            idx = int(choice) - 1\n",
    "            user_answers.append(q.options[idx])\n",
    "        except:\n",
    "            user_answers.append(\"INVALID\")\n",
    "        print()\n",
    "    return user_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce58c63",
   "metadata": {
    "papermill": {
     "duration": 0.006035,
     "end_time": "2025-04-19T16:28:14.036483",
     "exception": false,
     "start_time": "2025-04-19T16:28:14.030448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## evaluate_quiz Function\n",
    "\n",
    "This cell is responsible for evaluating the user's quiz answers and returning a score and a list of per-question results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a18c20e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:28:14.053776Z",
     "iopub.status.busy": "2025-04-19T16:28:14.053293Z",
     "iopub.status.idle": "2025-04-19T16:28:14.059182Z",
     "shell.execute_reply": "2025-04-19T16:28:14.058143Z"
    },
    "papermill": {
     "duration": 0.016623,
     "end_time": "2025-04-19T16:28:14.060846",
     "exception": false,
     "start_time": "2025-04-19T16:28:14.044223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_quiz(quiz: list[QuizQuestion], user_answers: list[str]):\n",
    "    \"\"\"\n",
    "    Compare the user's answers to the correct ones. \n",
    "    Return a score and a list of per-question results.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    results = []\n",
    "    for i, question in enumerate(quiz):\n",
    "        if i < len(user_answers) and user_answers[i] == question.answer:\n",
    "            score += 1\n",
    "            results.append(f\"Question {i+1}: Correct!\")\n",
    "        else:\n",
    "            results.append(f\"Question {i+1}: Incorrect! Correct answer: {question.answer}\")\n",
    "    return score, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd2d71",
   "metadata": {
    "papermill": {
     "duration": 0.005892,
     "end_time": "2025-04-19T16:28:14.073130",
     "exception": false,
     "start_time": "2025-04-19T16:28:14.067238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Socratic Dialogue Evaluation Prompt and Rating Enum\n",
    "\n",
    "This cell defines the prompt used to evaluate the AI's performance in a Socratic dialogue. It outlines the evaluation criteria, rating rubric, and evaluation steps.\n",
    "\n",
    "The `SocraticRating` enum class provides a structured way to represent the evaluation ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3b66f99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:28:14.087011Z",
     "iopub.status.busy": "2025-04-19T16:28:14.086637Z",
     "iopub.status.idle": "2025-04-19T16:28:14.092226Z",
     "shell.execute_reply": "2025-04-19T16:28:14.091158Z"
    },
    "papermill": {
     "duration": 0.014302,
     "end_time": "2025-04-19T16:28:14.093781",
     "exception": false,
     "start_time": "2025-04-19T16:28:14.079479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the evaluation prompt for Socratic Dialogue\n",
    "SOCRATIC_EVAL_PROMPT = \"\"\"\n",
    "Instruction: You are an expert evaluator assessing the quality of responses generated by an AI-based Socratic tutor. The purpose of a Socratic tutor is to help learners arrive at solutions through guided questioning and critical thinking, promoting self-discovery rather than directly providing the answer.\n",
    "\n",
    "Please ignore any irrelevant topics. Only evaluate the following input and AI response with respect to the subject: \"{subject}\".\n",
    "\n",
    "Carefully read the user input and the AI generated response provided. Evaluate the quality of the response based on the criteria outlined below. Provide step by step explanations for your rating, strictly using the given Rating Rubric.\n",
    "\n",
    "Metric Definition: You will evaluate the response quality specifically based on its effectiveness as a Socratic dialogue. The response should encourage active thinking, self-discovery, and critical analysis, guiding learners to derive answers independently rather than explicitly providing solutions.\n",
    "\n",
    "Criteria:\n",
    "\n",
    "- Socratic Engagement:    \n",
    "    - Does the response primarily use questions and prompts that stimulate critical thinking and encourage the learner to reflect deeply?\n",
    "    - Does the response avoid directly providing the answer, guiding the learner toward self-discovery instead?\n",
    "        \n",
    "- Relevance and Groundedness:\n",
    "    - Does the response clearly relate to the user's input and the learning goal (specifically regarding \"{subject}\")?\n",
    "    - Does the response avoid introducing irrelevant or external information?\n",
    "        \n",
    "- Clarity and Understandability:\n",
    "    - Is the response phrased clearly and understandably, making it easy for the learner to grasp and engage with the questions posed?\n",
    "        \n",
    "- Depth and Thoughtfulness:\n",
    "    - Does the response show thoughtful consideration of the learner’s level of understanding, adapting complexity accordingly?\n",
    "    - Does it encourage deeper analysis rather than superficial engagement?\n",
    "\n",
    "Rating Rubric:\n",
    "- 5 (Excellent): The response thoroughly embodies the Socratic method, strongly encourages critical thinking, is highly relevant, clear, and thoughtfully engages the learner.\n",
    "    \n",
    "- 4 (Good): The response effectively uses Socratic questioning, is relevant, clear, and generally thoughtful, though it might have minor areas for improved engagement or depth.\n",
    "    \n",
    "- 3 (Moderate): The response shows basic Socratic engagement and relevance but lacks clarity or sufficient depth to fully encourage meaningful reflection or self-discovery.\n",
    "    \n",
    "- 2 (Poor): The response is minimally Socratic, provides too much direct guidance or answers explicitly, lacks clarity, or has limited relevance.\n",
    "    \n",
    "- 1 (Very Poor): The response fails to utilize the Socratic method, directly provides answers without prompting self-discovery, is unclear, irrelevant, or not useful in stimulating reflection.\n",
    "\n",
    "Evaluation Steps:\n",
    "STEP 1: Carefully analyze the AI generated response according to each criterion (Socratic Engagement, Relevance and Groundedness, Clarity and Understandability, Depth and Thoughtfulness).\n",
    "STEP 2: Assign a rating from the rubric and provide a clear step by step explanation justifying your evaluation.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# A structured enum class for Socratic Dialogue ratings\n",
    "class SocraticRating(enum.Enum):\n",
    "    EXCELLENT = '5'\n",
    "    GOOD = '4'\n",
    "    ADEQUATE = '3'\n",
    "    POOR = '2'\n",
    "    VERY_POOR = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a01198",
   "metadata": {
    "papermill": {
     "duration": 0.005791,
     "end_time": "2025-04-19T16:28:14.105853",
     "exception": false,
     "start_time": "2025-04-19T16:28:14.100062",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Function to Evaluate Socratic Dialogue\n",
    "\n",
    "This function evaluates the AI's responses in a Socratic dialogue using the Gemini API. It takes the user's message, AI's response, and the subject as input, and returns a verbose evaluation and a structured rating.\n",
    "\n",
    "This function uses a two-step approach:\n",
    "\n",
    "1.  Generates a detailed evaluation based on the `SOCRATIC_EVAL_PROMPT`.\n",
    "2.  Converts the final score from the evaluation into a `SocraticRating` enum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83d80cb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:28:14.119291Z",
     "iopub.status.busy": "2025-04-19T16:28:14.118913Z",
     "iopub.status.idle": "2025-04-19T16:28:14.124952Z",
     "shell.execute_reply": "2025-04-19T16:28:14.123794Z"
    },
    "papermill": {
     "duration": 0.014932,
     "end_time": "2025-04-19T16:28:14.126904",
     "exception": false,
     "start_time": "2025-04-19T16:28:14.111972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_socratic_dialogue(user_message, ai_response, subject):\n",
    "    \"\"\"Evaluates the AI's response in a Socratic dialogue using generate_content.\"\"\"\n",
    "    \n",
    "    formatted_prompt = SOCRATIC_EVAL_PROMPT.format(\n",
    "        subject=subject,\n",
    "        user_message=user_message,\n",
    "        ai_response=ai_response\n",
    "    )\n",
    "    \n",
    "    # Step 1: Generate verbose evaluation\n",
    "    first_response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=formatted_prompt,\n",
    "        config=types.GenerateContentConfig()\n",
    "    )\n",
    "    verbose_eval = first_response.text\n",
    "    \n",
    "    # Step 2: Convert final score to enum\n",
    "    second_prompt = (\n",
    "        \"You produced the following evaluation text:\\n\\n\"\n",
    "        f\"{verbose_eval}\\n\\n\"\n",
    "        \"Now, please convert the final score into an enum from 1 to 5. \"\n",
    "        \"Use only the rating rubric's standard enumerations: 1, 2, 3, 4, or 5.\"\n",
    "    )\n",
    "    structured_output_config = types.GenerateContentConfig(\n",
    "        response_mime_type=\"text/x.enum\",\n",
    "        response_schema=SocraticRating\n",
    "    )\n",
    "    second_response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=second_prompt,\n",
    "        config=structured_output_config\n",
    "    )\n",
    "    structured_eval = second_response.parsed\n",
    "    \n",
    "    return verbose_eval, structured_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5bdc36",
   "metadata": {
    "papermill": {
     "duration": 0.006304,
     "end_time": "2025-04-19T16:28:14.139860",
     "exception": false,
     "start_time": "2025-04-19T16:28:14.133556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Main Workflow - start_learning_session\n",
    "\n",
    "This cell orchestrates the entire learning session: generating the quiz, administering it, evaluating the results, and initiating the Socratic dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e8a123d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:28:14.153699Z",
     "iopub.status.busy": "2025-04-19T16:28:14.153361Z",
     "iopub.status.idle": "2025-04-19T16:28:14.162462Z",
     "shell.execute_reply": "2025-04-19T16:28:14.161321Z"
    },
    "papermill": {
     "duration": 0.018041,
     "end_time": "2025-04-19T16:28:14.164198",
     "exception": false,
     "start_time": "2025-04-19T16:28:14.146157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def start_learning_session(topic: str, doQuiz: bool, turn_on_verbose: bool):\n",
    "    \"\"\"\n",
    "    - Generates a quiz for the given topic (If wanted)\n",
    "    - Administers the quiz (collects user answers) (If wanted)\n",
    "    - Evaluates the quiz (If wanted)\n",
    "    - Appends a summary of quiz results to the conversation (If wanted)\n",
    "    - Begins a Socratic dialogue with ask_agent\n",
    "    \"\"\"\n",
    "    \n",
    "    if(doQuiz):\n",
    "        # Step 1: Generate the quiz\n",
    "        quiz = generate_quiz(topic)\n",
    "\n",
    "        # Step 2: Present quiz to user & collect answers\n",
    "        user_answers = administer_quiz(quiz)\n",
    "\n",
    "        # Step 3: Evaluate the quiz\n",
    "        score, results = evaluate_quiz(quiz, user_answers)\n",
    "        summary = f\"Quiz Results ({score}/{len(quiz)})\\n\" + \"\\n\".join(results)\n",
    "\n",
    "    \n",
    "        # Step 4: Append quiz summary to chat\n",
    "        history.append(ChatTurn(role=\"user\", content=summary))\n",
    "\n",
    "        # Now let's begin the Socratic conversation referencing the quiz result\n",
    "        prompt_for_agent = (\n",
    "            f\"Based on this quiz result, let's begin the Socratic dialogue on {topic}.\\n\"\n",
    "            f\"Quiz summary:\\n{summary}.\\n\"\n",
    "            \"This is where the user currently stands with their knowledge on the topic. \"\n",
    "            \"We can use this to identify potential weaknesses and address them in a socratic way. \"\n",
    "            \"Please keep your responses concise, focusing on short, guided questions. \"\n",
    "            \"Start by trying to understand what the user knows already about the topic, \"\n",
    "            \"unless the user explicitly requests more detail.\"\n",
    "        )\n",
    "\n",
    "        # Send the initial prompt\n",
    "        initial_response = ask_agent(prompt_for_agent)\n",
    "        print(f\"{Colors.GREEN}AI: {initial_response.text}{Colors.RESET}\")\n",
    "\n",
    "    else:    \n",
    "        prompt_for_agent = (\n",
    "           f\"We are ready to start our socratic dialogue with following topic {topic}\"\n",
    "        )\n",
    "\n",
    "        initial_response = ask_agent(prompt_for_agent)\n",
    "        print(f\"{Colors.GREEN}AI: {initial_response.text}{Colors.RESET}\")\n",
    "\n",
    "    # Conversation loop\n",
    "    while True:\n",
    "        user_input = input(f\"{Colors.BLUE}User Input:{Colors.RESET} \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(f\"{Colors.GREEN}AI:Goodbye!{Colors.RESET}\")\n",
    "            break\n",
    "        \n",
    "        # Get the AI response\n",
    "        agent_response = ask_agent(user_input)\n",
    "        print(f\"{Colors.GREEN}AI: {agent_response.text}{Colors.RESET}\")\n",
    "\n",
    "        # Evaluate the AI response\n",
    "        verbose_eval, structured_eval = eval_socratic_dialogue(\n",
    "            user_message=user_input,\n",
    "            ai_response=agent_response.text,\n",
    "            subject=topic\n",
    "        )\n",
    "\n",
    "        # Display evaluation\n",
    "        print(f\"\\n{Colors.RED}Evaluation:{Colors.RESET}\")\n",
    "        if(turn_on_verbose):\n",
    "            print(f\"{Colors.RED}{verbose_eval}{Colors.RESET}\")\n",
    "        print(f\"{Colors.RED}Rating: {structured_eval.value}{Colors.RESET}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b9de57",
   "metadata": {
    "papermill": {
     "duration": 0.006,
     "end_time": "2025-04-19T16:28:14.177149",
     "exception": false,
     "start_time": "2025-04-19T16:28:14.171149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Example Learning Sessions\n",
    "\n",
    "This cell demonstrates how to start a learning session with different topics. The `start_learning_session` function initiates the quiz, dialogue, and evaluation process.\n",
    "\n",
    "**Important:** The cell below initiates an interactive session which uses input(). This will fail during non-interactive Kaggle \"Save Version\" runs. So I have left this part commented out. Uncomment these lines when running the notebook interactively yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2885da10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:28:14.190780Z",
     "iopub.status.busy": "2025-04-19T16:28:14.190407Z",
     "iopub.status.idle": "2025-04-19T16:28:14.194219Z",
     "shell.execute_reply": "2025-04-19T16:28:14.193137Z"
    },
    "papermill": {
     "duration": 0.012446,
     "end_time": "2025-04-19T16:28:14.195882",
     "exception": false,
     "start_time": "2025-04-19T16:28:14.183436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- IMPORTANT ---\n",
    "# The line below initiates an interactive session which uses input().\n",
    "# This will fail during non-interactive Kaggle 'Save Version' runs.\n",
    "# Uncomment the line ONLY when running the notebook interactively yourself.\n",
    "# -----------------\n",
    "\n",
    "# start_learning_session(\"How does an MP (McCulloch-Pitts) neuron work?\", True, False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 97258,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15.354655,
   "end_time": "2025-04-19T16:28:14.922934",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-19T16:27:59.568279",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
